{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGjcDf_SRi2F",
        "outputId": "1a2f472d-21ec-4570-f3f3-157f696c8cbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "import textwrap\n",
        "import keras\n",
        "import random\n",
        "import pandas as pd\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import progressbar\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coUKlmQvRpiV",
        "outputId": "5bfd0b8d-a228-4ee3-a0e3-895c5b0c426f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Thesis/train_data/ILDC_multi.csv')\n",
        "\n",
        "df_train = df.query(\" split=='train' \")\n",
        "df_test = df.query(\" split=='test' \")\n",
        "df_dev = df.query(\" split=='dev' \")"
      ],
      "metadata": {
        "id": "Uc_E8a-wRzYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = '/content/drive/MyDrive/Thesis/calibration/saved_model_multi_2560'\n",
        "device = torch.device('cuda')\n",
        "model = AutoModelForSequenceClassification.from_pretrained(output_dir, output_hidden_states=True, trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "qUt-sNyrSI-X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80b48dd2-4122-4cf5-aed9-dc418656f751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSGBertForSequenceClassification(\n",
              "  (bert): LSGBertModel(\n",
              "    (embeddings): LSGBertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(4096, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (global_embeddings): Embedding(512, 768)\n",
              "    )\n",
              "    (encoder): LSGBertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x LSGBertLayer(\n",
              "          (attention): LSGAttention(\n",
              "            (self): LSGSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (attention): LSGAttentionProduct(\n",
              "                (attention): BaseAttentionProduct(\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (full_attention): BaseAttentionProduct(\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def att_masking(input_ids):\n",
        "  attention_masks = []\n",
        "  for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_masks.append(att_mask)\n",
        "  return attention_masks"
      ],
      "metadata": {
        "id": "UYspY-SKSAZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grouped_input_ids_reverse(all_toks):\n",
        "    splitted_toks = []\n",
        "    total_tokens = len(all_toks)\n",
        "    chunk_size = 510\n",
        "    overlap_size = 100\n",
        "\n",
        "    end = total_tokens\n",
        "    if (end<101):\n",
        "      splitted_toks.insert(0, all_toks)\n",
        "    else:\n",
        "      while end > 100:\n",
        "        start = max(0, end - chunk_size)\n",
        "        chunk = all_toks[start:end]\n",
        "        splitted_toks.insert(0, chunk)\n",
        "        if start!=0:\n",
        "          end = start + overlap_size\n",
        "        else:\n",
        "          end=0\n",
        "\n",
        "    CLS = tokenizer.cls_token\n",
        "    SEP = tokenizer.sep_token\n",
        "    e_sents = []\n",
        "    for chunk in splitted_toks:\n",
        "        chunk = [CLS] + chunk + [SEP]\n",
        "        encoded_sent = tokenizer.convert_tokens_to_ids(chunk)\n",
        "        e_sents.append(encoded_sent)\n",
        "\n",
        "    e_sents = pad_sequences(e_sents, maxlen=512, value=0, dtype=\"long\", padding=\"pre\")\n",
        "    att_masks = att_masking(e_sents)\n",
        "    return e_sents, att_masks\n"
      ],
      "metadata": {
        "id": "IFG0UTt7XIvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output_for_one_vec(input_id, att_mask):\n",
        "  input_ids = torch.tensor(input_id)\n",
        "  att_masks = torch.tensor(att_mask)\n",
        "  input_ids = input_ids.unsqueeze(0)\n",
        "  att_masks = att_masks.unsqueeze(0)\n",
        "  model.eval()\n",
        "  input_ids = input_ids.to(device)\n",
        "  att_masks = att_masks.to(device)\n",
        "  with torch.no_grad():\n",
        "      outputs = model(input_ids=input_ids, attention_mask=att_masks)\n",
        "\n",
        "  vec = outputs[\"hidden_states\"][12][0][0]\n",
        "  vec = vec.detach().cpu().numpy()\n",
        "  return vec"
      ],
      "metadata": {
        "id": "9rWXx17nSaOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_np_files_for_emb(dataf, tokenizer):\n",
        "  all_docs = []\n",
        "  for i in progressbar.progressbar(range(len(dataf['text']))):\n",
        "    text = dataf['text'].iloc[i]\n",
        "    toks = tokenizer.tokenize(text)\n",
        "    if(len(toks) > 10000):\n",
        "      toks = toks[len(toks)-10000:]\n",
        "\n",
        "    splitted_input_ids, splitted_att_masks = grouped_input_ids_reverse(toks)\n",
        "\n",
        "    vecs = []\n",
        "    for index,ii in enumerate(splitted_input_ids):\n",
        "      vecs.append(get_output_for_one_vec(ii, splitted_att_masks[index]))\n",
        "\n",
        "    one_doc = np.asarray(vecs)\n",
        "    all_docs.append(one_doc)\n",
        "\n",
        "  all_docs = np.asarray(all_docs)\n",
        "  return all_docs"
      ],
      "metadata": {
        "id": "sRX4JOdySw34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLS\n",
        "path_val_npy_file = \"/content/drive/MyDrive/Thesis/Models_whole_data/case_explanation/LSGBERT_bigru_att_full/LSGBERT_npy_files_cls_multi_full/LSGBERT_cls_train\"\n",
        "vecs_train = generate_np_files_for_emb(df_train, tokenizer)\n",
        "np.save(path_val_npy_file, vecs_train)\n",
        "\n",
        "print('npy file train saved')"
      ],
      "metadata": {
        "id": "WwexK5jpS0Fy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd7f24c-a973-4bac-e56d-c079ef7bceed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0% (0 of 32305) |                      | Elapsed Time: 0:00:00 ETA:  --:--:--Token indices sequence length is longer than the specified maximum sequence length for this model (55443 > 4096). Running this sequence through the model will result in indexing errors\n",
            " 93% (30266 of 32305) |################  | Elapsed Time: 3:39:11 ETA:   0:05:19"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CLS\n",
        "path_val_npy_file = \"/content/drive/MyDrive/Thesis/Models_whole_data/case_explanation/LSGBERT_bigru_att_full/LSGBERT_npy_files_cls_multi_full/LSGBERT_cls_dev\"\n",
        "vecs_dev = generate_np_files_for_emb(df_dev, tokenizer)\n",
        "np.save(path_val_npy_file, vecs_dev)\n",
        "\n",
        "print('npy file dev saved')"
      ],
      "metadata": {
        "id": "_pNbMRfJTWmB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c85e65e8-d059-40b5-a106-a4166115747e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0% (0 of 994) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--Token indices sequence length is longer than the specified maximum sequence length for this model (7726 > 4096). Running this sequence through the model will result in indexing errors\n",
            "100% (994 of 994) |######################| Elapsed Time: 0:08:45 Time:  0:08:45\n",
            "<ipython-input-8-c2dcbbfdf16d>:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  all_docs = np.asarray(all_docs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "npy file dev saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CLS\n",
        "path_val_npy_file = \"/content/drive/MyDrive/Thesis/Models_whole_data/case_explanation/LSGBERT_bigru_att_full/LSGBERT_npy_files_cls_multi_full/LSGBERT_cls_test\"\n",
        "vecs_test = generate_np_files_for_emb(df_test, tokenizer)\n",
        "np.save(path_val_npy_file, vecs_test)\n",
        "\n",
        "print('npy file test saved')"
      ],
      "metadata": {
        "id": "qjawYgIiTXBI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d71a4d96-6372-433c-fc38-119260ea0111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% (1517 of 1517) |####################| Elapsed Time: 0:13:49 Time:  0:13:49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "npy file test saved\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-c2dcbbfdf16d>:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  all_docs = np.asarray(all_docs)\n"
          ]
        }
      ]
    }
  ]
}