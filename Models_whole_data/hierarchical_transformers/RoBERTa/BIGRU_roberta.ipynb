{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO7OpfpQWWjqBWUv84SUQOq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nBeZYdAu_2Cb","executionInfo":{"status":"ok","timestamp":1684753462005,"user_tz":-120,"elapsed":17132,"user":{"displayName":"Tesi Polito","userId":"06322488515562524575"}},"outputId":"cc2d4ef2-32af-478e-e04a-5c76a63c4367"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from keras import Sequential\n","from keras.utils import Sequence\n","from keras.layers import LSTM, Dense, Masking, GRU\n","import numpy as np\n","import keras\n","from keras.utils import np_utils\n","from keras import optimizers\n","from keras.models import Sequential, Model\n","from keras.layers import Embedding, Dense, Input, concatenate, Layer, Lambda, Dropout, Activation\n","import datetime\n","from datetime import datetime\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, TensorBoard\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.models import load_model\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import numpy as np\n","from numpy import load\n","import pandas as pd\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","\n","\n","np.random.seed(1337)# setting the random seed value"],"metadata":{"id":"4nHJ0kHs__gS","executionInfo":{"status":"ok","timestamp":1684753470887,"user_tz":-120,"elapsed":8887,"user":{"displayName":"Tesi Polito","userId":"06322488515562524575"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["dataset = pd.read_csv('/content/drive/MyDrive/Thesis/train_data/ILDC_multi.csv') # loading dataset"],"metadata":{"id":"eEZAuiBMAAi1","executionInfo":{"status":"ok","timestamp":1684753482092,"user_tz":-120,"elapsed":11225,"user":{"displayName":"Tesi Polito","userId":"06322488515562524575"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# path to transformer generated chunk embeddings\n","path_transformer_chunk_embeddings_train = '/content/drive/MyDrive/Thesis/Models_whole_data/transformers_sentence_level/RoBERTa_multi_5ep/RoBERTa_npy_files_cls_multi_5ep/RoBERTa_cls_train.npy' \n","path_transformer_chunk_embeddings_dev = '/content/drive/MyDrive/Thesis/Models_whole_data/transformers_sentence_level/RoBERTa_multi_5ep/RoBERTa_npy_files_cls_multi_5ep/RoBERTa_cls_dev.npy'\n","path_transformer_chunk_embeddings_test = '/content/drive/MyDrive/Thesis/Models_whole_data/transformers_sentence_level/RoBERTa_multi_5ep/RoBERTa_npy_files_cls_multi_5ep/RoBERTa_cls_test.npy'"],"metadata":{"id":"6qL7RejoAZUo","executionInfo":{"status":"ok","timestamp":1684753482093,"user_tz":-120,"elapsed":20,"user":{"displayName":"Tesi Polito","userId":"06322488515562524575"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# loading the chunk embeddings\n","x_train0 = load(path_transformer_chunk_embeddings_train, allow_pickle = True)\n","x_dev0 = load(path_transformer_chunk_embeddings_dev, allow_pickle= True)\n","x_test0 = load(path_transformer_chunk_embeddings_test, allow_pickle= True)"],"metadata":{"id":"2-_9HfjWCZ_y","executionInfo":{"status":"ok","timestamp":1684753497525,"user_tz":-120,"elapsed":15450,"user":{"displayName":"Tesi Polito","userId":"06322488515562524575"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# loading the corresponding label for each case in dataset\n","dev = dataset.loc[dataset['split'] == 'dev'] \n","train = dataset.loc[dataset['split'] == 'train'] \n","test = dataset.loc[dataset['split'] == 'test'] \n","\n","y_train0 = []\n","for i in range(train.shape[0]):\n","    y_train0.append(train.loc[i,'label'])  \n","    \n","y_dev0 = []\n","for i in range(dev.shape[0]):\n","    y_dev0.append(dev.loc[i+32305,'label'])\n","\n","y_test0 = []\n","for i in range(test.shape[0]):\n","    y_test0.append(test.loc[i+33299,'label'])"],"metadata":{"id":"OnFEN7GMH0RM","executionInfo":{"status":"ok","timestamp":1684753497951,"user_tz":-120,"elapsed":429,"user":{"displayName":"Tesi Polito","userId":"06322488515562524575"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from keras import layers\n","# Input layer to convert into required tensor shape\n","text_input = Input(shape=(None,768,), dtype='float32', name='text')\n","# Masking layer to mask the padded values\n","l_mask = layers.Masking(mask_value=-99.)(text_input)\n","# After masking we encoded the vector using 2 bidirectional GRU's\n","encoded_text = layers.Bidirectional(GRU(100,return_sequences=True))(l_mask)\n","encoded_text1 = layers.Bidirectional(GRU(100,))(encoded_text)\n","# Added a dense layer after encoding\n","out_dense = layers.Dense(30, activation='relu')(encoded_text1)\n","# And we add a sigmoid classifier on top\n","out = layers.Dense(1, activation='sigmoid')(out_dense)\n","# At model instantiation, we specify the input and the output:\n","model = Model(text_input, out)\n","model.compile(optimizer='Adam',\n","              loss='binary_crossentropy',\n","              metrics=['acc'])\n","model.summary()"],"metadata":{"id":"z6LM4pdNJmBi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684753505992,"user_tz":-120,"elapsed":8043,"user":{"displayName":"Tesi Polito","userId":"06322488515562524575"}},"outputId":"6f0de9c2-3a9d-4130-8341-8911bd2caba1"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," text (InputLayer)           [(None, None, 768)]       0         \n","                                                                 \n"," masking (Masking)           (None, None, 768)         0         \n","                                                                 \n"," bidirectional (Bidirectiona  (None, None, 200)        522000    \n"," l)                                                              \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 200)              181200    \n"," nal)                                                            \n","                                                                 \n"," dense (Dense)               (None, 30)                6030      \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 31        \n","                                                                 \n","=================================================================\n","Total params: 709,261\n","Trainable params: 709,261\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["num_sequences = len(x_train0)\n","batch_size = 32 \n","batches_per_epoch =  int(num_sequences/batch_size)\n","num_features= 768\n","def train_generator(): # function to generate batches of corresponding batch size\n","    x_list= x_train0\n","    y_list =  y_train0\n","    # Generate batches\n","    while True:\n","        for b in range(batches_per_epoch):\n","            longest_index = (b + 1) * batch_size - 1\n","            timesteps = len(max(x_train0[:(b + 1) * batch_size][-batch_size:], key=len))\n","            x_train = np.full((batch_size, timesteps, num_features), -99.)\n","            y_train = np.zeros((batch_size,  1))\n","            # padding the vectors with respect to the maximum sequence of each batch and not the whole training data\n","            for i in range(batch_size):\n","                li = b * batch_size + i\n","                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n","                y_train[i] = y_list[li]\n","            yield x_train, y_train"],"metadata":{"id":"BoK5NCQP_cO-","executionInfo":{"status":"ok","timestamp":1684753505993,"user_tz":-120,"elapsed":10,"user":{"displayName":"Tesi Polito","userId":"06322488515562524575"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["num_sequences_val = len(x_dev0)\n","batch_size_val = 32\n","batches_per_epoch_val = int(num_sequences_val/batch_size_val)\n","num_features= 768\n","def val_generator():# Similar function to generate validation batches\n","    x_list= x_dev0\n","    y_list =  y_dev0\n","    # Generate batches\n","    while True:\n","        for b in range(batches_per_epoch_val):\n","            longest_index = (b + 1) * batch_size_val - 1\n","            timesteps = len(max(x_dev0[:(b + 1) * batch_size_val][-batch_size_val:], key=len))\n","            x_train = np.full((batch_size_val, timesteps, num_features), 0)\n","            y_train = np.zeros((batch_size_val,  1))\n","            # padding the vectors with respect to the maximum sequence of each batch and not the whole validation data\n","            for i in range(batch_size_val):\n","                li = b * batch_size_val + i\n","                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n","                y_train[i] = y_list[li]\n","            yield x_train, y_train"],"metadata":{"id":"xO_tFYok_fIx","executionInfo":{"status":"ok","timestamp":1684753505993,"user_tz":-120,"elapsed":8,"user":{"displayName":"Tesi Polito","userId":"06322488515562524575"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Setting the callback and training the model\n","call_reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=2, verbose=2,\n","                                mode='auto', min_delta=0.01, cooldown=0, min_lr=0)\n","\n","model.fit(train_generator(), steps_per_epoch=batches_per_epoch, epochs=3,\n","                    validation_data=val_generator(), validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JZ3NMoG5_kQx","executionInfo":{"status":"ok","timestamp":1684753622278,"user_tz":-120,"elapsed":116293,"user":{"displayName":"Tesi Polito","userId":"06322488515562524575"}},"outputId":"3523ba26-9402-4cc1-8041-e569ceeb5eaa"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","1009/1009 [==============================] - 58s 35ms/step - loss: 0.4742 - acc: 0.7686 - val_loss: 0.6676 - val_acc: 0.6804 - lr: 0.0010\n","Epoch 2/3\n","1009/1009 [==============================] - 29s 29ms/step - loss: 0.4577 - acc: 0.7783 - val_loss: 0.6637 - val_acc: 0.5726 - lr: 0.0010\n","Epoch 3/3\n","1007/1009 [============================>.] - ETA: 0s - loss: 0.4464 - acc: 0.7841\n","Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n","1009/1009 [==============================] - 29s 29ms/step - loss: 0.4463 - acc: 0.7840 - val_loss: 0.6577 - val_acc: 0.6058 - lr: 0.0010\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fac7c414610>"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["num_sequences_test = len(x_test0)\n","batch_size_test = 32\n","batches_per_epoch_test = int(num_sequences_test/batch_size_test) + 1\n","num_features= 768\n","def test_generator(): # function to generate batches of corresponding batch size\n","    x_list= x_test0\n","    y_list =  y_test0\n","    # Generate batches\n","    while True:\n","        for b in range(batches_per_epoch_test):\n","            if(b == batches_per_epoch_test-1): # An extra if else statement just to manage the last batch as it's size might not be equal to batch size \n","              longest_index = num_sequences_test - 1\n","              timesteps = len(max(x_test0[:longest_index + 1][-batch_size_test:], key=len))\n","              x_train = np.full((longest_index - b*batch_size_test, timesteps, num_features), -99.)\n","              y_train = np.zeros((longest_index - b*batch_size_test,  1))\n","              for i in range(longest_index - b*batch_size_test):\n","                  li = b * batch_size_test + i\n","                  x_train[i, 0:len(x_list[li]), :] = x_list[li]\n","                  y_train[i] = y_list[li]\n","            else:\n","                longest_index = (b + 1) * batch_size_test - 1\n","                timesteps = len(max(x_test0[:(b + 1) * batch_size_test][-batch_size_test:], key=len))\n","                x_train = np.full((batch_size_test, timesteps, num_features), -99.)\n","                y_train = np.zeros((batch_size_test,  1))\n","                # padding the vectors with respect to the maximum sequence of each batch and not the whole test data\n","                for i in range(batch_size_test):\n","                    li = b * batch_size_test + i\n","                    x_train[i, 0:len(x_list[li]), :] = x_list[li]\n","                    y_train[i] = y_list[li]\n","            yield x_train, y_train"],"metadata":{"id":"TW_0Rk3u-inx","executionInfo":{"status":"ok","timestamp":1684753622279,"user_tz":-120,"elapsed":21,"user":{"displayName":"Tesi Polito","userId":"06322488515562524575"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# evaluating on the test data\n","model.evaluate_generator(test_generator(), steps= batches_per_epoch_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_gfgQPwD-0Jy","executionInfo":{"status":"ok","timestamp":1684753628465,"user_tz":-120,"elapsed":6206,"user":{"displayName":"Tesi Polito","userId":"06322488515562524575"}},"outputId":"1739c65e-d91f-4bba-91e8-a4c37498f0be"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-12-8e91bbf97f2e>:2: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n","  model.evaluate_generator(test_generator(), steps= batches_per_epoch_test)\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.5385285019874573, 0.7236147522926331]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# defining a function which calculates various metrics such as micro and macro precision, accuracy and f1\n","def metrics_calculator(preds, test_labels):\n","    cm = confusion_matrix(test_labels, preds)\n","    TP = []\n","    FP = []\n","    FN = []\n","    for i in range(0,2):\n","        summ = 0\n","        for j in range(0,2):\n","            if(i!=j):\n","                summ=summ+cm[i][j]\n","\n","        FN.append(summ)\n","    for i in range(0,2):\n","        summ = 0\n","        for j in range(0,2):\n","            if(i!=j):\n","                summ=summ+cm[j][i]\n","\n","        FP.append(summ)\n","    for i in range(0,2):\n","        TP.append(cm[i][i])\n","    precision = []\n","    recall = []\n","    for i in range(0,2):\n","        precision.append(TP[i]/(TP[i] + FP[i]))\n","        recall.append(TP[i]/(TP[i] + FN[i]))\n","\n","    macro_precision = sum(precision)/2\n","    macro_recall = sum(recall)/2\n","    micro_precision = sum(TP)/(sum(TP) + sum(FP))\n","    micro_recall = sum(TP)/(sum(TP) + sum(FN))\n","    micro_f1 = (2*micro_precision*micro_recall)/(micro_precision + micro_recall)\n","    macro_f1 = (2*macro_precision*macro_recall)/(macro_precision + macro_recall)\n","    return macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1"],"metadata":{"id":"X55xIhE2BgVi","executionInfo":{"status":"ok","timestamp":1684753628466,"user_tz":-120,"elapsed":18,"user":{"displayName":"Tesi Polito","userId":"06322488515562524575"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["model.evaluate(val_generator(), steps= batches_per_epoch_val)"],"metadata":{"id":"1bcSI0KHGPgt","executionInfo":{"status":"ok","timestamp":1684753629147,"user_tz":-120,"elapsed":698,"user":{"displayName":"Tesi Polito","userId":"06322488515562524575"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a31c6c90-3969-4894-9b78-24f1f4abe1cb"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["31/31 [==============================] - 1s 16ms/step - loss: 0.6577 - acc: 0.6058\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.6576849818229675, 0.6058467626571655]"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# getting the predicted labels on the dev data\n","preds = model.predict(val_generator(), steps= batches_per_epoch_val)\n","y_pred_dev = preds > 0.5\n","\n","# Calculating all metrics on dev data predicted label\n","print(metrics_calculator(y_pred_dev, y_dev0[:-2]))"],"metadata":{"id":"v9x4FhCbBkQz","executionInfo":{"status":"ok","timestamp":1684753634791,"user_tz":-120,"elapsed":5649,"user":{"displayName":"Tesi Polito","userId":"06322488515562524575"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c930cf72-85fd-4efb-9813-cb6851cc56dc"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["31/31 [==============================] - 5s 10ms/step\n","(0.7050539014575836, 0.6058467741935484, 0.6516964095979015, 0.6058467741935484, 0.6058467741935484, 0.6058467741935484)\n"]}]},{"cell_type":"code","source":["# getting the predicted labels on the test data\n","preds = model.predict_generator(test_generator(), steps= batches_per_epoch_test)\n","y_pred = preds > 0.5\n","\n","# Calculating all metrics on test data predicted label\n","print(metrics_calculator(y_pred, y_test0[:-1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X_d93yOn-3g_","executionInfo":{"status":"ok","timestamp":1684753640890,"user_tz":-120,"elapsed":6105,"user":{"displayName":"Tesi Polito","userId":"06322488515562524575"}},"outputId":"447410f6-344a-4e1f-eee0-28e37e2c2fa7"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-16-129cc16c0af6>:2: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n","  preds = model.predict_generator(test_generator(), steps= batches_per_epoch_test)\n"]},{"output_type":"stream","name":"stdout","text":["(0.7277926891615542, 0.7233415425851311, 0.7255602892457945, 0.7236147757255936, 0.7236147757255936, 0.7236147757255937)\n"]}]}]}